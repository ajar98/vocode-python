---
title: "Using transcribers"
description: "How to configure and use transcribers for speech recognition in your application."
---

# Overview

Transcribers are used to configure the speech recognition of your application. In this guide, we will show you how to configure and use transcribers in Vocode.

## Supported Transcribers

Vocode currently supports the following transcribers:

1. Deepgram
2. Google
3. Assembly AI
4. Whisper CPP
5. Rev AI
6. Azure

These transcribers are defined using their respective configuration classes, which are subclasses of the `TranscriberConfig` class.

# Configuring Transcribers

To use a transcriber, you need to create a configuration object for the transcriber you want to use. Here are some examples of how to create configuration objects for different transcribers:

### Example 1: Using Deepgram with a phone call

```python
from vocode.streaming.telephony.hosted.inbound_call_server import InboundCallServer
from vocode.streaming.models.transcriber import DeepgramTranscriberConfig, PunctuationEndpointingConfig

server = InboundCallServer(
    ...
    transcriber_config=DeepgramTranscriberConfig.from_telephone_input_device(
      endpointing_config=DeepgramEndpointingConfig()
    ),
    ...
)
```

In this example, the `DeepgramTranscriberConfig.from_telephone_input_device()` method is used to create a configuration object for the Deepgram transcriber. The method hardcodes some values like the `sampling_rate`, `audio_encoding`, and `chunk_size` for compatibility with telephone input devices.

### Example 2: Using Deepgram in `StreamingConversation` locally

```python
from vocode.streaming.models.transcriber import DeepgramTranscriberConfig, PunctuationEndpointingConfig
from vocode.streaming import StreamingConversation

async def main():
    microphone_input, speaker_output = create_microphone_input_and_speaker_output(
        streaming=True, use_default_devices=False
    )

    conversation = StreamingConversation(
        output_device=speaker_output,
        transcriber=DeepgramTranscriber(
            DeepgramTranscriberConfig.from_input_device(
                microphone_input, endpointing_config=DeepgramEndpointingConfig()
            )
        ),
        ...
    )
```

In this example, the `DeepgramTranscriberConfig.from_input_device()` method is used to create a configuration object for the Deepgram transcriber for use in a local `StreamingConversation`.
The method takes a `microphone_input` object as an argument and extracts the `sampling_rate`, `audio_encoding`, and `chunk_size` from the input device.

## Endpointing

Endpointing is the process of understanding when someone has finished speaking. The `EndpointingConfig` controls how this is done. There are a couple of different ways to configure endpointing:

We provide `DeepgramEndpointingConfig()` which has some reasonable defaults and knobs to suit most use-cases (but only works with the Deepgram transcriber).

```
class DeepgramEndpointingConfig(EndpointingConfig, type="deepgram"):  # type: ignore
    vad_threshold_ms: int = 500
    utterance_cutoff_ms: int = 1000
    time_silent_config: Optional[TimeSilentConfig] = Field(default_factory=TimeSilentConfig)
    use_single_utterance_endpointing_for_first_utterance: bool = False
```

- `vad_threshold_ms`: translates to [Deepgram's `endpointing` feature](https://developers.deepgram.com/docs/endpointing#enable-feature)
- `utterance_cutoff_ms`: uses [Deepgram's Utterance End features](https://developers.deepgram.com/docs/utterance-end)
- `time_silent_config`: is a Vocode specific parameter that marks an utterance final if we haven't seen any new words in X seconds
- `use_single_utterance_endpointing_for_first_utterance`: Uses `is_final` instead of `speech_final` for endpointing for the first utterance (works really well for outbound conversations, where the user's first utterance is something like "Hello?") - see [this doc on Deepgram](https://developers.deepgram.com/docs/understand-endpointing-interim-results) for more info.

Endpointing is highly use-case specific - building a realistic experience for this greatly depends on the person speaking to the AI. Here are few paradigms that we've used to help you along the way:

- Time-based endpointing: This method considers the speaker to be finished when there is a certain duration of silence.
- Punctuation-based endpointing: This method considers the speaker to be finished when there is a certain duration of silence after a punctuation mark.
